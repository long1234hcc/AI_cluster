{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f962c807-aed2-4617-94b3-95b980b1cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_input_path = \"data_eReport_map_1308_(122k_remove).xlsx\"\n",
    "file_out_put = \"group_e_report.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c31801-e292-4c5b-a8f8-fb8430f591d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file\n",
      "Load model\n",
      "Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 11247/11247 [06:37<00:00, 28.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359879, 359879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarity Matrix:   0%|          | 1/360 [16:41<99:51:13, 1001.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_ids\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Use the sparse matrix-based cosine similarity grouping function\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m group_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_similar_vectors_cosine_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_standardized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.92\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame to store results\u001b[39;00m\n\u001b[1;32m    121\u001b[0m result_df \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m, in \u001b[0;36mgroup_similar_vectors_cosine_sparse\u001b[0;34m(all_embeddings, threshold, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(similarity\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 101\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m similarity[m, n] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m    102\u001b[0m                     similarity_matrix[i \u001b[38;5;241m+\u001b[39m m, j \u001b[38;5;241m+\u001b[39m n] \u001b[38;5;241m=\u001b[39m similarity[m, n]\n\u001b[1;32m    104\u001b[0m group_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m embedding_count\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import lil_matrix  # Import sparse matrix\n",
    "df = pd.read_excel(file_input_path)\n",
    "\n",
    "\n",
    "## Sử dụng dưới 100k dòng\n",
    "\n",
    "def normalized(txt):\n",
    "    if txt is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return str(txt).lower().strip()\n",
    "\n",
    "print(\"Read file\")\n",
    "# Load the updated data with text descriptions\n",
    "file_input_path = file_input_path  \n",
    "file_out_put = file_out_put  \n",
    "test_data = pd.read_excel(file_input_path)\n",
    "test_data[\"product_name\"] = test_data[\"product_name\"].apply(normalized)\n",
    "print(\"Load model\")\n",
    "\n",
    "# Initialize the tokenizer and model for inflat-e5-large-v2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-large-v2\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-large-v2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=300)\n",
    "\n",
    "def preprocess_texts(dataloader, model, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Processing Batches'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            features.append(embeddings)\n",
    "\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "# Extract features\n",
    "print(\"Embedding...\")\n",
    "texts = test_data[\"product_name\"].tolist()\n",
    "dataset = TextDataset(texts)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "features = preprocess_texts(dataloader, model, device)\n",
    "\n",
    "# Optionally, reduce dimensionality with PCA\n",
    "pca = PCA(n_components=300)\n",
    "features_reduced = pca.fit_transform(features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features_reduced)\n",
    "\n",
    "# Group similar vectors using cosine similarity with sparse matrix\n",
    "def group_similar_vectors_cosine_sparse(all_embeddings, threshold=0.75, batch_size=1000):\n",
    "    embeddings = torch.tensor(all_embeddings, dtype=torch.float32)\n",
    "    embedding_count = len(all_embeddings)\n",
    "    similarity_matrix = lil_matrix((embedding_count, embedding_count))\n",
    "    print(similarity_matrix.shape)\n",
    "\n",
    "    # Compute similarity matrix in batches\n",
    "    for i in tqdm(range(0, embedding_count, batch_size), desc='Computing Similarity Matrix'):\n",
    "        for j in range(0, embedding_count, batch_size):\n",
    "            batch_embeddings_i = embeddings[i:i + batch_size]\n",
    "            batch_embeddings_j = embeddings[j:j + batch_size]\n",
    "            similarity = F.cosine_similarity(batch_embeddings_i.unsqueeze(1), batch_embeddings_j.unsqueeze(0), dim=-1)\n",
    "            similarity = similarity.numpy()\n",
    "            for m in range(similarity.shape[0]):\n",
    "                for n in range(similarity.shape[1]):\n",
    "                    if similarity[m, n] >= threshold:\n",
    "                        similarity_matrix[i + m, j + n] = similarity[m, n]\n",
    "\n",
    "    group_ids = [-1] * embedding_count\n",
    "    current_group_id = 0\n",
    "\n",
    "    for i in tqdm(range(embedding_count), desc='Grouping Vectors'):\n",
    "        if group_ids[i] == -1:\n",
    "            group_ids[i] = current_group_id\n",
    "            for j in range(i + 1, embedding_count):\n",
    "                if group_ids[j] == -1 and similarity_matrix[i, j] >= threshold:\n",
    "                    group_ids[j] = current_group_id\n",
    "            current_group_id += 1\n",
    "\n",
    "    return group_ids\n",
    "\n",
    "# Use the sparse matrix-based cosine similarity grouping function\n",
    "group_ids = group_similar_vectors_cosine_sparse(features_standardized, threshold=0.92, batch_size=1000)\n",
    "\n",
    "# Create a new DataFrame to store results\n",
    "result_df = test_data.copy()\n",
    "result_df['group_text'] = group_ids\n",
    "\n",
    "# Save to Excel\n",
    "result_df.to_excel(file_out_put, index=False)\n",
    "\n",
    "print(\"Finished processing and saved results to\", file_out_put)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14a90da-1b71-4ee6-ad2b-e7e1b4dba988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161093, 20)\n",
      "Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/11247 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing Batches: 100%|██████████| 11247/11247 [06:39<00:00, 28.12it/s]\n",
      "Computing Similarity Matrix:   1%|          | 2/360 [05:22<16:00:38, 161.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 105\u001b[0m\n\u001b[1;32m    101\u001b[0m             current_group_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_ids\n\u001b[0;32m--> 105\u001b[0m group_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_similar_vectors_cosine_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_standardized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.92\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m result_df \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    108\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m group_ids\n",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m, in \u001b[0;36mgroup_similar_vectors_cosine_sparse\u001b[0;34m(all_embeddings, threshold, batch_size)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i, embedding_count, batch_size):\n\u001b[1;32m     84\u001b[0m     batch_embeddings_j \u001b[38;5;241m=\u001b[39m embeddings[j:j \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 85\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_embeddings_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_embeddings_j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m similarity\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     87\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(similarity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import lil_matrix, coo_matrix\n",
    "\n",
    "# Optimized Filtering\n",
    "df = pd.read_excel(file_input_path)\n",
    "categories = [\"Nhà cửa & Đời sống\", \"Thời Trang Nữ\", \"Sắc Đẹp\", \"Điện Thoại & Phụ Kiện\", \n",
    "              \"Mẹ & Bé\", \"Thời Trang Nam\", \"Thực phẩm và đồ uống\", \"Phụ Kiện Thời Trang\", \n",
    "              \"Thiết Bị Điện Gia Dụng\"]\n",
    "df = df[(df['remove_edited'] != \"x\") & (df['category_name'].isin(categories))]\n",
    "print(df.shape)\n",
    "\n",
    "def normalized(txt):\n",
    "    return str(txt).lower().strip() if txt is not None else ''\n",
    "\n",
    "# print(\"Read file\")\n",
    "# test_data = pd.read_excel(file_input_path)\n",
    "# test_data[\"name\"] = test_data[\"name\"].apply(normalized)\n",
    "# print(\"Load model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-large-v2\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-large-v2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=300)\n",
    "\n",
    "def preprocess_texts(dataloader, model, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Processing Batches'):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            features.append(embeddings)\n",
    "\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "# Embedding\n",
    "print(\"Embedding...\")\n",
    "texts = test_data[\"name\"].tolist()\n",
    "dataset = TextDataset(texts)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "features = preprocess_texts(dataloader, model, device)\n",
    "\n",
    "# PCA Reduction\n",
    "pca = PCA(n_components=300)\n",
    "features_reduced = pca.fit_transform(features)\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features_reduced)\n",
    "\n",
    "def group_similar_vectors_cosine_sparse(all_embeddings, threshold=0.75, batch_size=1000):\n",
    "    embeddings = torch.tensor(all_embeddings, dtype=torch.float32)\n",
    "    embedding_count = len(all_embeddings)\n",
    "    similarity_matrix = lil_matrix((embedding_count, embedding_count))\n",
    "\n",
    "    # Compute similarity matrix in batches\n",
    "    for i in tqdm(range(0, embedding_count, batch_size), desc='Computing Similarity Matrix'):\n",
    "        batch_embeddings_i = embeddings[i:i + batch_size]\n",
    "        for j in range(i, embedding_count, batch_size):\n",
    "            batch_embeddings_j = embeddings[j:j + batch_size]\n",
    "            similarity = F.cosine_similarity(batch_embeddings_i.unsqueeze(1), batch_embeddings_j.unsqueeze(0), dim=-1)\n",
    "            similarity = similarity.numpy()\n",
    "            indices = np.argwhere(similarity >= threshold)\n",
    "            for index in indices:\n",
    "                similarity_matrix[i + index[0], j + index[1]] = similarity[index[0], index[1]]\n",
    "                if i != j:\n",
    "                    similarity_matrix[j + index[1], i + index[0]] = similarity[index[0], index[1]]\n",
    "\n",
    "    group_ids = np.full(embedding_count, -1, dtype=int)\n",
    "    current_group_id = 0\n",
    "\n",
    "    for i in tqdm(range(embedding_count), desc='Grouping Vectors'):\n",
    "        if group_ids[i] == -1:\n",
    "            group_ids[i] = current_group_id\n",
    "            connected_indices = similarity_matrix[i].nonzero()[1]\n",
    "            group_ids[connected_indices] = current_group_id\n",
    "            current_group_id += 1\n",
    "\n",
    "    return group_ids\n",
    "\n",
    "group_ids = group_similar_vectors_cosine_sparse(features_standardized, threshold=0.8, batch_size=1000)\n",
    "\n",
    "result_df = test_data.copy()\n",
    "result_df['group_text'] = group_ids\n",
    "\n",
    "result_df.to_excel(file_out_put, index=False)\n",
    "print(\"Finished processing and saved results to\", file_out_put)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f8e757-9246-4ce7-8773-b0f845e947c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings:\n",
      "Apple iPhone 13: [1.  0.8 0.9 0.7]\n",
      "Apple iPhone 14: [1.   0.85 0.88 0.68]\n",
      "Samsung Galaxy S21: [0.2 0.1 0.3 0.2]\n",
      "Samsung Galaxy S22: [0.25 0.15 0.28 0.18]\n",
      "Google Pixel 6: [0.6 0.9 0.4 0.7]\n",
      "Google Pixel 7: [0.58 0.88 0.42 0.72]\n",
      "Sony Xperia 5: [0.05 0.2  0.1  0.3 ]\n",
      "Sony Xperia 1: [0.07 0.18 0.12 0.28]\n",
      "clusterer HDBSCAN(min_cluster_size=2)\n",
      "labels [0 0 1 1 0 0 1 1]\n",
      "\n",
      "Phân cụm sản phẩm:\n",
      "Group 0:\n",
      "- Apple iPhone 13\n",
      "- Apple iPhone 14\n",
      "- Google Pixel 6\n",
      "- Google Pixel 7\n",
      "Group 1:\n",
      "- Samsung Galaxy S21\n",
      "- Samsung Galaxy S22\n",
      "- Sony Xperia 5\n",
      "- Sony Xperia 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Giả định danh sách tên sản phẩm\n",
    "lst_product_name = [\n",
    "    \"Apple iPhone 13\",\n",
    "    \"Apple iPhone 14\",\n",
    "    \"Samsung Galaxy S21\",\n",
    "    \"Samsung Galaxy S22\",\n",
    "    \"Google Pixel 6\",\n",
    "    \"Google Pixel 7\",\n",
    "    \"Sony Xperia 5\",\n",
    "    \"Sony Xperia 1\"\n",
    "]\n",
    "\n",
    "# Giả định embedding (thủ công) với các vector có 4 chiều\n",
    "# Các sản phẩm cùng hãng sẽ có các embedding gần nhau hơn\n",
    "lst_embedings_product_name = np.array([\n",
    "    [1.0, 0.8, 0.9, 0.7],  # Apple iPhone 13\n",
    "    [1.0, 0.85, 0.88, 0.68], # Apple iPhone 14\n",
    "    [0.2, 0.1, 0.3, 0.2],   # Samsung Galaxy S21\n",
    "    [0.25, 0.15, 0.28, 0.18], # Samsung Galaxy S22\n",
    "    [0.6, 0.9, 0.4, 0.7],   # Google Pixel 6\n",
    "    [0.58, 0.88, 0.42, 0.72], # Google Pixel 7\n",
    "    [0.05, 0.2, 0.1, 0.3],  # Sony Xperia 5\n",
    "    [0.07, 0.18, 0.12, 0.28]  # Sony Xperia 1\n",
    "])\n",
    "\n",
    "# In ra các embedding để xem xét\n",
    "print(\"Embeddings:\")\n",
    "for name, emb in zip(lst_product_name, lst_embedings_product_name):\n",
    "    print(f\"{name}: {emb}\")\n",
    "\n",
    "# Xây dựng FAISS index\n",
    "d = lst_embedings_product_name.shape[1]  # số chiều của embedding (4)\n",
    "index = faiss.IndexFlatL2(d)     # dùng chỉ số L2 (chuẩn Euclidean)\n",
    "index.add(lst_embedings_product_name)     # thêm các embedding vào index\n",
    "\n",
    "# Sử dụng HDBSCAN để phân cụm\n",
    "clusterer = HDBSCAN(min_cluster_size=2)  # Số lượng sản phẩm trong cụm tối thiểu là 2\n",
    "labels = clusterer.fit_predict(lst_embedings_product_name)\n",
    "\n",
    "\n",
    "# Tạo danh sách các nhóm sản phẩm\n",
    "grouped_products = {}\n",
    "for label, product_name in zip(labels, lst_product_name):\n",
    "    if label not in grouped_products:\n",
    "        grouped_products[label] = []\n",
    "    grouped_products[label].append(product_name)\n",
    "\n",
    "# Hiển thị kết quả phân cụm\n",
    "print(\"\\nPhân cụm sản phẩm:\")\n",
    "for label, products in grouped_products.items():\n",
    "    print(f\"Group {label}:\")\n",
    "    for product in products:\n",
    "        print(f\"- {product}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378ca219-9505-4cb0-a3c3-d2d95636b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 31.36 GB\n",
      "Available memory: 22.11 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Kiểm tra tổng dung lượng RAM và dung lượng RAM khả dụng\n",
    "mem = psutil.virtual_memory()\n",
    "total_memory = mem.total / (1024 ** 3)  # Đổi sang GB\n",
    "available_memory = mem.available / (1024 ** 3)  # Đổi sang GB\n",
    "\n",
    "print(f\"Total memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available memory: {available_memory:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b79b5-8186-46ff-90a0-65e52172f708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9289466d-560a-4cd4-8632-f194083bd7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kem dưỡng ẩm\n"
     ]
    }
   ],
   "source": [
    "tesst_txt = \"kem olay dưỡng ẩm\"\n",
    "print( tesst_txt.replace(\"olay \",\"\")      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c9f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
